---
title: "Dimensionality Reduction PCA, SVD Implementation"
excerpt: "Let's study dimensionality reduction."
categories:
 - study
tags:
 - recomender system
 - linear algebra
use_math: true
last_modified_at: "2020-06-25"
toc: true
toc_sticky: true
toc_label: "On this page"
toc_icon: "cog"
header:
 overlay_image: /assets/images/teaser.jpg
 overlay_filter: 0.5
---

<div class="prompt input_prompt">
In&nbsp;[1]:
</div>

<div class="input_area" markdown="1">

```python
import numpy as np
from numpy import linalg as la

np.set_printoptions(precision=3)
```

</div>

# PCA

<img src="https://i.imgur.com/Uv2dlsH.gif" width="400">

## Step 1. Centering
변수(행)별로 평균을 $0$으로 centering한 행렬 $X$를 만듬 <br>
(좌표계의 원점이 평균 벡터와 일치하도록 만듬)

$n$: the number of data(sample) <br>
$p$: the dimension of each data

<div class="prompt input_prompt">
In&nbsp;[2]:
</div>

<div class="input_area" markdown="1">

```python
n, p = 5, 3
X = np.random.randint(10, size = (n, p)).astype('float64')
print("* 원본 data")
print(X)
```

</div>

{:.output_stream}

```
* 원본 data
[[8. 7. 7.]
 [9. 1. 8.]
 [5. 9. 0.]
 [6. 5. 1.]
 [1. 2. 6.]]

```

<div class="prompt input_prompt">
In&nbsp;[3]:
</div>

<div class="input_area" markdown="1">

```python
print("* 각 dimension 별 평균")
print(np.mean(X, axis=0))
X -= np.mean(X, axis=0)
print(X)
```

</div>

{:.output_stream}

```
* 각 dimension 별 평균
[5.8 4.8 4.4]
[[ 2.2  2.2  2.6]
 [ 3.2 -3.8  3.6]
 [-0.8  4.2 -4.4]
 [ 0.2  0.2 -3.4]
 [-4.8 -2.8  1.6]]

```

## Step 2. Find Covariance Matrix 

공분산행렬은 다음 식으로 만들 수 있음
$$
\Sigma = cov(X)= \frac{X^T X} {n − 1} \propto X^T X
$$

<div class="prompt input_prompt">
In&nbsp;[4]:
</div>

<div class="input_area" markdown="1">

```python
C = np.cov(X, rowvar=False)
print(C)
```

</div>

{:.output_stream}

```
[[ 9.7  0.7  3.1]
 [ 0.7 11.2 -7.9]
 [ 3.1 -7.9 13.3]]

```

## Step 3. Find Eigen Values and Vectors

공분산행렬(`C`)을 기반으로 고유값(`l`)과 고유벡터(`principal_axes`) 구하기

<div class="prompt input_prompt">
In&nbsp;[5]:
</div>

<div class="input_area" markdown="1">

```python
l, principal_axes = la.eig(C)
print(l)
print(principal_axes)
```

</div>

{:.output_stream}

```
[ 3.227 10.418 20.554]
[[-0.379  0.908  0.176]
 [ 0.668  0.4   -0.627]
 [ 0.64   0.12   0.759]]

```

고유값을 높은 순으로 정렬하고, 이에 대응하는 고유벡터와 순서를 맞춤

<div class="prompt input_prompt">
In&nbsp;[6]:
</div>

<div class="input_area" markdown="1">

```python
idx = l.argsort()[::-1]
idx
```

</div>




{:.output_data_text}

```
array([2, 1, 0])
```



<div class="prompt input_prompt">
In&nbsp;[7]:
</div>

<div class="input_area" markdown="1">

```python
l, principal_axes = l[idx], principal_axes[:, idx]
print(l)
print(principal_axes)
```

</div>

{:.output_stream}

```
[20.554 10.418  3.227]
[[ 0.176  0.908 -0.379]
 [-0.627  0.4    0.668]
 [ 0.759  0.12   0.64 ]]

```

<div class="prompt input_prompt">
In&nbsp;[8]:
</div>

<div class="input_area" markdown="1">

```python
# np.matmul(principal_axes.T, principal_axes) 
```

</div>

## Step 4. Dimensionality Reduction

차원축소 예 (고유값을 기준으로 가장 큰 $k$개의 고유 벡터 선택)
* principal axis(principal_compoents) 를 구함

<div class="prompt input_prompt">
In&nbsp;[9]:
</div>

<div class="input_area" markdown="1">

```python
# k = 2 
print(principal_axes[:, :2])
```

</div>

{:.output_stream}

```
[[ 0.176  0.908]
 [-0.627  0.4  ]
 [ 0.759  0.12 ]]

```

* principal axis 를 기반으로 원본데이터 X에 대한 principal components 를 구함 
  <br>그리고 **top 2 개**의 components 들에 대해서 dimensionality reduction을 수행

<div class="prompt input_prompt">
In&nbsp;[10]:
</div>

<div class="input_area" markdown="1">

```python
mapped_data = X.dot(principal_axes)
print(mapped_data)
```

</div>

{:.output_stream}

```
[[ 0.979  3.192  2.301]
 [ 5.678  1.818 -1.445]
 [-6.113  0.426  0.29 ]
 [-2.669 -0.147 -2.12 ]
 [ 2.125 -5.289  0.974]]

```

<div class="prompt input_prompt">
In&nbsp;[11]:
</div>

<div class="input_area" markdown="1">

```python
mapped_data_reduced = X.dot(principal_axes[:, :2])
print(mapped_data_reduced)
```

</div>

{:.output_stream}

```
[[ 0.979  3.192]
 [ 5.678  1.818]
 [-6.113  0.426]
 [-2.669 -0.147]
 [ 2.125 -5.289]]

```

<div class="prompt input_prompt">
In&nbsp;[12]:
</div>

<div class="input_area" markdown="1">

```python
# 새로운 변수 z1 과 z2 (5개의 데이터에대한 각각의 dimension component를 갖고있는 vector)
z1 = mapped_data_reduced[:,0]
z2 = mapped_data_reduced[:,1]
print(z1)
print(z2)
```

</div>

{:.output_stream}

```
[ 0.979  5.678 -6.113 -2.669  2.125]
[ 3.192  1.818  0.426 -0.147 -5.289]

```

### Example 2

<div class="prompt input_prompt">
In&nbsp;[13]:
</div>

<div class="input_area" markdown="1">

```python
n, d = 4, 2
X = np.array([[0,-2],[3,-3],[2,0],[-1,1]]).astype('float64')
print(X)
```

</div>

{:.output_stream}

```
[[ 0. -2.]
 [ 3. -3.]
 [ 2.  0.]
 [-1.  1.]]

```

<div class="prompt input_prompt">
In&nbsp;[14]:
</div>

<div class="input_area" markdown="1">

```python
# 각 row마다 빼진다. 알아서 broadcasting 되어 계산됨
X -= np.mean(X, axis=0)
print(X)
```

</div>

{:.output_stream}

```
[[-1. -1.]
 [ 2. -2.]
 [ 1.  1.]
 [-2.  2.]]

```

$$ \Sigma =cov(X)= \frac{X^T X}{n−1} $$

Covariance Matrix `C`(or $\Sigma$) can be computed by $\frac{X^TX}{n - 1}$

<div class="prompt input_prompt">
In&nbsp;[15]:
</div>

<div class="input_area" markdown="1">

```python
print(np.matmul(X.T,X) / (n - 1))
```

</div>

{:.output_stream}

```
[[ 3.333 -2.   ]
 [-2.     3.333]]

```

<div class="prompt input_prompt">
In&nbsp;[16]:
</div>

<div class="input_area" markdown="1">

```python
C = np.cov(X, rowvar=False)
print(C)
```

</div>

{:.output_stream}

```
[[ 3.333 -2.   ]
 [-2.     3.333]]

```

<div class="prompt input_prompt">
In&nbsp;[17]:
</div>

<div class="input_area" markdown="1">

```python
l, principal_axes = la.eig(C)
print(l)
print(principal_axes)
```

</div>

{:.output_stream}

```
[5.333 1.333]
[[ 0.707  0.707]
 [-0.707  0.707]]

```

<div class="prompt input_prompt">
In&nbsp;[18]:
</div>

<div class="input_area" markdown="1">

```python
idx = l.argsort()[::-1]
l, principal_axes = l[idx], principal_axes[:, idx]
print(l)
print(principal_axes)
```

</div>

{:.output_stream}

```
[5.333 1.333]
[[ 0.707  0.707]
 [-0.707  0.707]]

```

<div class="prompt input_prompt">
In&nbsp;[19]:
</div>

<div class="input_area" markdown="1">

```python
mapped_data = X.dot(principal_axes)
print(mapped_data)
```

</div>

{:.output_stream}

```
[[-2.220e-16 -1.414e+00]
 [ 2.828e+00 -4.441e-16]
 [ 2.220e-16  1.414e+00]
 [-2.828e+00  4.441e-16]]

```

# SVD

SVD(Sigular Value Decomposition) means that <br>
$R$ can be decomposed by $U \in \mathbb{R}^{n \times k}, \Sigma \in \mathbb{R}^{k \times k}, V \in \mathbb{R}^{d \times k}$ mathmatically, where $k \le d $. <br>

$$ R=U \Sigma V^T $$
* <span style="color:red">Note that $RR^T$ 또는 $R^TR$ 은 **symmetric matrix**이므로, **eigen decomposition**이 가능</span>
* $U$는 $RR^T$ (즉, **$R$의 공분산 행렬) 의 고유벡터들**
* $V$는 $R^TR$ (즉, **$R^T$의 공분산 행렬) 의 고유벡터들**
* $\Sigma \Sigma^T$ 또는 $\Sigma^T \Sigma= \lambda$ 
  따라서, singular value (특이값) $\sigma = \sqrt{\lambda}$
    
> approach: 
  $RR^T$ 또는 $R^TR$ 은 **symmetric square matrix** 이므로 <br>
  **eigen decomposition이 가능**하고,
  <br>이것을 이용하여, $U, V, \lambda$ 를 구한다.

<div class="prompt input_prompt">
In&nbsp;[20]:
</div>

<div class="input_area" markdown="1">

```python
user, item = 5, 3
R = np.random.randint(10, size = (user, item)).astype('float')
# user, item = 4, 2
# R = np.array([[2,3],[1,4],[0,0],[0,0]]).astype('float')
# R = np.array([[1,0,0,0,0],[0,0,2,0,3],[0,0,0,0,0],[0,2,0,0,0]])
print("* 원본 data")
print(R)
# print("* 각 dimension 별 평균")
# print(np.mean(R, axis=0))
# R -= np.mean(R, axis=0)
# print(R)
```

</div>

{:.output_stream}

```
* 원본 data
[[2. 9. 5.]
 [6. 8. 1.]
 [1. 3. 5.]
 [3. 8. 4.]
 [3. 5. 5.]]

```

## Use Libriary 

<div class="prompt input_prompt">
In&nbsp;[21]:
</div>

<div class="input_area" markdown="1">

```python
# we now perform singular value decomposition of X
# "economy size" (or "thin") SVD
# U, s, Vt = la.svd(R, full_matrices=True)
U, s, Vt = la.svd(R, full_matrices=False)
V = Vt.T
print(s)
S = np.diag(s)
print(U)
print(V)
print(S)
```

</div>

{:.output_stream}

```
[18.964  5.339  2.423]
[[-0.544 -0.207  0.635]
 [-0.484  0.758 -0.299]
 [-0.266 -0.553 -0.365]
 [-0.497  0.003  0.236]
 [-0.391 -0.276 -0.564]]
[[-0.365  0.517 -0.774]
 [-0.817  0.221  0.533]
 [-0.447 -0.827 -0.342]]
[[18.964  0.     0.   ]
 [ 0.     5.339  0.   ]
 [ 0.     0.     2.423]]

```

<div class="prompt input_prompt">
In&nbsp;[22]:
</div>

<div class="input_area" markdown="1">

```python
print(R)
```

</div>

{:.output_stream}

```
[[2. 9. 5.]
 [6. 8. 1.]
 [1. 3. 5.]
 [3. 8. 4.]
 [3. 5. 5.]]

```

<div class="prompt input_prompt">
In&nbsp;[23]:
</div>

<div class="input_area" markdown="1">

```python
print(np.matmul(np.matmul(U, S), V.T))
```

</div>

{:.output_stream}

```
[[2. 9. 5.]
 [6. 8. 1.]
 [1. 3. 5.]
 [3. 8. 4.]
 [3. 5. 5.]]

```

Trucated SVD(즉 dimensionality reduction을 함): 

<div class="prompt input_prompt">
In&nbsp;[24]:
</div>

<div class="input_area" markdown="1">

```python
k = 2 
np.matmul(np.matmul(U[:,:k], S[:k,:k]), V.T[:k,:])
```

</div>




{:.output_data_text}

```
array([[3.191, 8.181, 5.525],
       [5.439, 8.386, 0.752],
       [0.315, 3.471, 4.698],
       [3.443, 7.695, 4.196],
       [1.941, 5.728, 4.533]])
```



## Compute Numerically

recall that ... <br>
* Note that $RR^T$ 또는 $R^TR$ 은 **symmetric matrix**이므로, **eigen decomposition**이 가능 
* $U$는 $RR^T$ 즉, **$R$의 공분산 행렬의 (`k`개의)고유벡터들**
* $V$는 $R^TR$ 즉, **$R^T$의 공분산 행렬의 (`k`개의)고유벡터들**

$$ cov(R)= \frac{R^T R}{n−1} \propto R^T R$$

### Step 1. Find Covarance of $R$ and $R^T$

<div class="prompt input_prompt">
In&nbsp;[25]:
</div>

<div class="input_area" markdown="1">

```python
print(np.matmul(R.T, R))
print(np.matmul(R, R.T))
C1 = np.matmul(R.T, R)
C2 = np.matmul(R, R.T)
```

</div>

{:.output_stream}

```
[[ 59. 108.  48.]
 [108. 243. 125.]
 [ 48. 125.  92.]]
[[110.  89.  54.  98.  76.]
 [ 89. 101.  35.  86.  63.]
 [ 54.  35.  35.  47.  43.]
 [ 98.  86.  47.  89.  69.]
 [ 76.  63.  43.  69.  59.]]

```

### Step 2. Find Eigen Values and Eigen Vectors of $Cov(R)$ or $Cov(R^T)$

**$Cov(R)$ 의 고유벡터들을 구하면 $V$가 된다**.

<div class="prompt input_prompt">
In&nbsp;[26]:
</div>

<div class="input_area" markdown="1">

```python
# # C1 is a symmetric matrix and so it can be diagonalized:
l, principal_axes = la.eig(C1)
# sort results wrt. eigenvalues
idx = l.argsort()[::-1]
l, principal_axes = l[idx], principal_axes[:, idx]
# the eigenvalues in decreasing order
print ("eigenvalues = \n", l)
# a matrix of eigenvectors (each column is an eigenvector)
V = principal_axes
print ("eigenvectors, which is same with V = \n", V)
# principal_components = R.dot(principal_axes)
# print ("principal_components = \n", principal_components)
```

</div>

{:.output_stream}

```
eigenvalues = 
 [359.629  28.5     5.87 ]
eigenvectors, which is same with V = 
 [[-0.365 -0.517 -0.774]
 [-0.817 -0.221  0.533]
 [-0.447  0.827 -0.342]]

```

### Step 3 Find $U, \Sigma$
$R = U\sum V^T$ 에서 양쪽 수식의 오른쪽에 $V$ 를 곱하면, 
<br> $RV = U\sum = [s_1 U_1, s_2 U_2,  ... ,s_r U_r] $ 인데 <br>
각 column마다 대응되는 singular value $s_i\vert_{i=1,2, ..., r}$를 나누어주면 
$U$ 를 구할 수 있다.

이때, singular value들은 V를 구할때 찾은 eigen values들이 된다.


<div class="prompt input_prompt">
In&nbsp;[27]:
</div>

<div class="input_area" markdown="1">

```python
singulars = np.sqrt(l)
print(singulars)
U = np.matmul(R, V)
# print(U)
for i, sing in enumerate(singulars):
    U[:,i] = U[:,i]/sing
print(U)
```

</div>

{:.output_stream}

```
[18.964  5.339  2.423]
[[-0.544  0.207  0.635]
 [-0.484 -0.758 -0.299]
 [-0.266  0.553 -0.365]
 [-0.497 -0.003  0.236]
 [-0.391  0.276 -0.564]]

```

<div class="prompt input_prompt">
In&nbsp;[28]:
</div>

<div class="input_area" markdown="1">

```python
S = np.diag(singulars)
S
```

</div>




{:.output_data_text}

```
array([[18.964,  0.   ,  0.   ],
       [ 0.   ,  5.339,  0.   ],
       [ 0.   ,  0.   ,  2.423]])
```



<div class="prompt input_prompt">
In&nbsp;[29]:
</div>

<div class="input_area" markdown="1">

```python
R
```

</div>




{:.output_data_text}

```
array([[2., 9., 5.],
       [6., 8., 1.],
       [1., 3., 5.],
       [3., 8., 4.],
       [3., 5., 5.]])
```



Sanity Check:Reconstruct $R$ by multiplication of $U, S, V^T$

<div class="prompt input_prompt">
In&nbsp;[30]:
</div>

<div class="input_area" markdown="1">

```python
np.matmul(np.matmul(U, S), V.T)
```

</div>




{:.output_data_text}

```
array([[2., 9., 5.],
       [6., 8., 1.],
       [1., 3., 5.],
       [3., 8., 4.],
       [3., 5., 5.]])
```



일반적으로, 어떤 rating matrix R을 가지고, 추정한 SVD값과의 RMSE는 최소가 됨이 증명 되어있다. 
<br>하지만, R에 missing value 가 있을 때에는 SVD를 구할수 없어 prediction 된 U, S, V를 찾아야한다. 
따라서, 이 predication된 SVD 는 latent factor 모델을 이용한 추천시스템에 사용된다. 

## Reference

[1] [korean blog - summary](https://www.fun-coding.org/recommend_basic5.html) <br>
[2] [Eigendecomposition of a Matrix - wiki](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) <br>
[3] [korean blog - Symmetric Matrix and Spectrum Theory](https://twlab.tistory.com/54) <br>

## Appendix

eigen values, eigen vectors들은 power iteration 을 반복하여 찾는 것이 가능하다. 

* power iteration 방법을 사용
$$
v_{t} = \frac{M v_{t-1}} { \lVert Mv_{t-1} \rVert }
$$

power iteration을 통해 첫번째로 수렴하는 v값이 가장 큰 eigen value와 대응되는 eigen vector이다. <br>
이때, largest eigen value는 
$$
v^T M v
$$ 이다. <br>
Gram schmidt process에 따라  $M$ 에서 largest igen vector 성분을 다음과 같이 제거하고, <br>
$$
M_{new} = M - \lambda v v^T 
$$
power iteration을 반복하면 두번째로 큰 eigen value와 대응하는 eigen vector를 찾을 수 있다.

[Theory: Gram schmidt process - wiki](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) <br>
[Korean blog: eigen values, vectors - darkprogrammer](https://darkpgmr.tistory.com/105) <br>
[Another korean blog: eigen values, vectors](https://twlab.tistory.com/46)

<div class="prompt input_prompt">
In&nbsp;[32]:
</div>

<div class="input_area" markdown="1">

```python
M = np.array([[1,1,1],[1,2,3],[1,3,5]]).astype("float")
print(M)

l, principal_axes = la.eig(M)
idx = l.argsort()[::-1]
l, principal_axes = l[idx], principal_axes[:, idx]
print(l) # eigen values
print(principal_axes) # eigen vectors
```

</div>

{:.output_stream}

```
[[1. 1. 1.]
 [1. 2. 3.]
 [1. 3. 5.]]
[ 7.162e+00  8.377e-01 -2.958e-16]
[[-0.218 -0.886  0.408]
 [-0.522 -0.248 -0.816]
 [-0.825  0.391  0.408]]

```

* power iteration in order to find eigen vector corresponding to the largest eigen value.

$$
v_{t} = \frac{M v_{t-1}} { \lVert Mv_{t-1} \rVert }
$$

<div class="prompt input_prompt">
In&nbsp;[33]:
</div>

<div class="input_area" markdown="1">

```python
elipslion = 1e-8
v = np.ones((np.size(M, axis=0),1))
iteration = 0
distance = 100
while distance > elipslion:
    print("iteration", iteration, "...")
    prev_v = v
    Mv = np.matmul(M,v)
    v = Mv/la.norm(Mv, 'fro')
    iteration = iteration + 1
    distance = np.linalg.norm(v-prev_v)
    print("distance = ",np.linalg.norm(v-prev_v))
print(v)
```

</div>

{:.output_stream}

```
iteration 0 ...
distance =  0.890429726059787
iteration 1 ...
distance =  0.04894577225762312
iteration 2 ...
distance =  0.005731964656124627
iteration 3 ...
distance =  0.0006704398749804303
iteration 4 ...
distance =  7.84167576513936e-05
iteration 5 ...
distance =  9.171868646813307e-06
iteration 6 ...
distance =  1.0727703710604327e-06
iteration 7 ...
distance =  1.2547456944453433e-07
iteration 8 ...
distance =  1.4675897110185787e-08
iteration 9 ...
distance =  1.716538713429754e-09
[[0.218]
 [0.522]
 [0.825]]

```

* Find largest eigen value by solving $Mv = \lambda v = v \lambda $

$$
\lambda = v^T M v
$$ 

<div class="prompt input_prompt">
In&nbsp;[34]:
</div>

<div class="input_area" markdown="1">

```python
l = np.matmul(np.matmul(v.T, M),v)
print(l)
```

</div>

{:.output_stream}

```
[[7.162]]

```

* eliminate principal eigen vector 
$$
M_{new} = M - \lambda v v^T 
$$

<div class="prompt input_prompt">
In&nbsp;[35]:
</div>

<div class="input_area" markdown="1">

```python
M = M - np.matmul(l*v,v.T)
print(M)
```

</div>

{:.output_stream}

```
[[ 0.658  0.184 -0.291]
 [ 0.184  0.051 -0.081]
 [-0.291 -0.081  0.128]]

```

* Find second largest eigen value

power iteration again <br>
then, we can get eigen vector corresponding to 2nd largeset eigen value 

<div class="prompt input_prompt">
In&nbsp;[36]:
</div>

<div class="input_area" markdown="1">

```python
elipslion = 1e-8
v = np.ones((np.size(M, axis=0),1))
iteration = 0
distance = 100
while distance > elipslion:
    print("iteration", iteration, "...")
    prev_v = v
    Mv = np.matmul(M,v)
    v = Mv/la.norm(Mv, 'fro')
    iteration = iteration + 1
    distance = np.linalg.norm(v-prev_v)
#     print("distance = ",np.linalg.norm(v-prev_v))
print(v)
```

</div>

{:.output_stream}

```
iteration 0 ...
iteration 1 ...
[[ 0.886]
 [ 0.248]
 [-0.391]]

```

## Futhermore

* MDS(multi-dimensional scaling)
* t-SNE(t-stochastic neighbor embeeding)
